# Исследование моделей и инфраструктуры

## Дисклеймер!

Чтобы определиться и сделать базовое  отсеивание моделей, нужно определиться с нагрузкой на них. Так как конкретных данных нет, я сознательно ввожу некие упрощения и допущения, чтобы в итоге получить не сколько точную цифру, сколько понимание, о каких порядках расходов будет идти речь.

### Вычисление нагрузки
Исходя из описания компании и количетва документации предположим, что в компании 300-400 сотрудников.
Если взять сценарий расширения компании в будущем (через год-два), а так же для удобства рассчетов, предположим, что целевое решение должно удовлетворять запросы 500 сотрудников ежедневно.

Также нужно учесть, что возможно вместо сотрудников обращаться к API будут автопроцессы для тестирования/оценки данных.
С учетом этого редположим, что каждый сотрудник/автопроцессе в среднем ежедневно обращается к системе 10 раз, соответственно в день максимальная нагрузка 5000 запросов.

Каждый запрос превращается в промт, состоящий примерно из 1000-2500 токенов. Соответсвенно, ежедневно система будет потреблять до 12.5млн токенов на вход.

Каждый промт содержит в себе системные настройки, чанки, иснтрукции, сам вопрос, пример ответа. В то же время ответ содержит какое-то резюме и ссылки на документы. Так как у нас нет понимания Мы введем соотношение, что на каждые 10 токенов вопроса будет 1 токен ответа. Значит ежедневная нагрузка будет составлять 1.25 млн output-токенов. Округлим до 15млн токенов ежедневно для целевого решения.

Умножим дневную нагрузку на 22 - количество рабочих дней в месяц, и получим картину **275 млн input-токенов и 27,5млн output-токенов в месяц**. 
Умножим дневную нагрузку на 250 - примерное количество рабочих дней в году, получим чуть меньше **3,125 млрд input-токенов  и 312.5млн output-токеновв год**.

### Определение цены этой нагрузки

Так как компания финско-эстонская, а клиенты - фины, шведы, скандинавы, то логично предположить, то ориентироваться нужно на европейско-американский рынок моделей. Но я специально добавлю обзор на российские и китайскую модели, ради понимания относительной стоимости в целом по миру.

Составим таблицу стоимости работы с различными AI решениями, с которыми можно работать по API.

Для рассчета использую формулу 
```js
function(input, output){
    let inputMonth = 275; //млн токенов в месяц
    let inputYear = 3125; //млн токенов в год
    return {
        perMonth: inputMonth * input + inputMonth * 0.1 * output,
        perYear: inputYear * input + inputYear * 0.1 * output
    }
}
```

|Название модели  |Стоимость в месяц|Cтоимость в год|
|---------------  |----------------:|--------------:|
|GPT-5.1          |          155 925|      1 771 875|
|GPT-4o mini      |            4 678|         53 156|
|Gemini 3 Ultra   |           89 100|      1 012 500|
|Gemini 2.0 Flash |            3 119|         35 438|
|Grok 4.1         |          155 925|      1 771 875|
|DeepSeek V3      |            8 465|         96 188|
|YandexGPT 5 Pro  |           48 114|        546 750|
|GigaChat 2 MAX   |           80 190|        911 250|

OPEX в год 53156руб. - 1 771 875руб.

Как видно из примеров, стоимость использования API значительно ниже даже средних зарплат IT-специалистов, которые могут развернуть подобные инструменты. 
Представленные модели от Google и OpenAI гарантируют, что модели не обучаются на пользовательских промтах, а так же сервисы соотсветсвуют международным стандартам ИБ (ISO/IEC), что позволяет использовать их для работы с внутренними документами. Как минимум их можно использовать на этапе MVP. Также стоит учесть, что если компания уже пользуется Asure от Microsoft, то у Microsoft есть преложения по предоставлению AI услуг в рамках экосистемы, поэтому это может довольно удобно для интеграции с текущей инфраструктурой.

Однако, если для безопасной работы API решения не подходят в силу каких-то причин, то можно рассмотреть self-hosted решения.

### Сравнение Self-host решений для корпоративного использования


Для обеспечения безопасности данных и независимости от внешних API, мы рассматриваем развертывание Open-Source моделей на собственных мощностях.
Вот три модели, которые являются лидерами в своих весовых категориях на текущий момент.

| Модель | Параметры | Качество (MMLU) | Особенности |
| :--- | :--- | :--- | :--- |
| **Llama 3.1 70B** | 70 млрд | ~86.0 | Золотой стандарт. Отлично следует сложным инструкциям и работает с RAG. |
| **DeepSeek-V3** | 671 млрд (MoE) | ~88.5 | Топовая производительность, сопоставимая с GPT-4o. Требует огромных ресурсов. |
| **Qwen 2.5 72B** | 72 млрд | ~85.0 | Лидер в кодинге и математике, очень стабильна в структурированных ответах. |

---

### Расчет необходимого оборудования (для целевой нагрузки)

Немного уточним наши подребности. Если мы рассчитывали, что пользователь + автопроцессы использовали 10 запросов в день, то реальная человеческая нагрузка может быть ниже, так как автопроцессы типа тестирования можно вывести во внерабочее время. Предлагаю допустить, что не каждый сотрудник каждый день будет пользоваться ИИ ботом, тогда можно предположить, что активная работа будет составлять 2-5 запросов в день на человека. 5 запросов в день на 8 часов. получим 0,625 запросов в час. 

0,625(запросов в час) * 500(сотрудников) * (2500(входящие токены) + 250(сухой ответ на 100слов (2-3 предложения и 3-4 сокращенными ссылками)) ) ~ **0,86 млн токенов в час.**. (но для упрощения возьмем 1 млн токенов в час, чтобы покрыть "нахлёсты" параллельных запросов).

Для получения производительности 1млн токенов в час, нам нужно ~17тыс токенов в час или ~280 токенов в секунду. 

Еще нужно понять количество необходимой памяти. В vRAM лежит сама модель и контекст. 
Размер модели считается так: количество параметров * размер одного параметра (квантование). 
Например, если взять Llama 3.1 70B в четырёхбитном квантавании (q4) то получим 70 000 000 000 * 4 / 8 / 1024 / 1024 / 1024 ≈ 32.6 ГБ. Среди доступных на HuggingFace вариантов этой модели встречаются модели размером 37-43гб

Так же необходимо учесть, что нужно в держать в памяти еще и KV(Key-Value) кеш запроса. Он рассчитывается так</br>
Память (байт)=2(ключ, значение)×(Слои модели)×(Длина ключа/значения)×(Размер байта)


Посчитаем для Llama 3.1 70B Q8, которую я видел на HF: 2 * 80 * 8192 * 1 ≈ 1,25 МБ на один токен.

1,25 * 2750 (На весь диалог) * 40 (условно одновременных запросов)  / 1024 ≈ 134 ГБ (для хранения кэша токенов 40 одновременных запросов)

Добавим вес модели 43 гб и про запас на систему и трансформеры добавим 3 гб, получим 180ГБ vRAM. **Ни одна потребительская карта такого объема не даст**. 

Чтобы модель помещалась на одну карту и не пришлось делать шардинг модели, нам нужна карта с 43гб+ vRAM. На текущий момент на рынке есть NVIDIA H100 80GB.

Для NVIDIA Link количество карт должно быть в степени двойки для эффективной работы, поэтому для эффективной работы и для запаса нужно хотя бы 4 таких карты.

#### Целевая конфигурация для Llama 3.1 70B / Qwen 2.5 72B:

Чтобы обеспечить комфортную скорость (30-50 токенов/сек на пользователя) при одновременных запросах, нам потребуется сервер с **4x или 8x NVIDIA H100/A100 (80GB)**, мощным CPU для предварительной обработки запросов, и большим объемом RAM.

#### Ориентировочная стоимость оборудования (РФ рынок):

| Компонент | Спецификация | Кол-во | Итоговая цена (ориентир) |
| :--- | :--- | :--- | :--- |
| **GPU** | NVIDIA H100 80GB | 4 шт | ~14 000 000 руб. |
| **Server Barebone** | 2x Xeon/EPYC, 512GB RAM | 1 шт | ~2 500 000 руб. |
| **Итого CAPEX** | | | **~16 500 000 руб.** |

---

### Сравнительный анализ: API vs Self-host

| Параметр | API решения | Self-host (Llama/Qwen на своем железе) |
| :--- | :--- | :--- |
| **Безопасность** | Риск утечки данных в облако | Данные не покидают контур компании |
| **Затраты (1-й год)** | ~40к - 1.8 млн руб. (только подписка) | ~17.5 млн руб. (CAPEX + внедрение + электричество) |
| **Затраты (2-й год+)** | Растут пропорционально трафику | Только обслуживание (~1-2 млн руб./год) |
| **Кастомизация** | Ограничена системным промтом | Полное дообучение (Fine-tuning) под специфику |


### Аренда GPU

В целом, возможно не покупать, но арендовать мощности GPU, что может оказаться выгоднее, однако подобный формат выходит за рамки этого решения.

### Гибрид 

Также мне видится, что можно классифицировать данные и разработать систему, при которой запросы по конфиденциальной информации уходят на урезанную self-hosted модель, а информация по более открытым данным отправляется по API. Мне видится, что такое разделение можно задать на уровне метаданных чанков. 

### Резюме

Переход на self-host оправдан только для удовлетворения критическим требованиями безопасности. Окупаемость относительно дорогих API (типа GPT-5) наступит через 10-12 месяцев. Относительно дешевых API (младшие модели) — никогда, оплата будет идти только за "приватность". Для проверки гипотезы self-host хватит локальных мощностей на ноутбуках (Macbook M-series 16гб+ RAM или игровые ноутбуки с видеокартой от 4гб), небольших моделей типа квантизированных Qwen3, Gema, Mistral. Если ноутбуков таких нет, то можно найти на бУ рынке недорогую игровую ПК сборку и оттестироваться на ней.

ВАЖНО! Нужно помнить, что у бесплатных моделей могут быть ограничения на коммерческую деятельность, поэтому перед стартом нужно удостовериться, что

Однако гораздо выгоднее запуститься сначала на "слабеньких" API, а потом перейти на "сильные". 

Для принятия решения по выбору следующих шагов необходимы детальные уточнения по классификации информации и требованиям к выявленным категориям.

### Полезные ссылки:

Калькулятор токенов https://token-counter.ru/ </br>
Статистика зарубежных моделей https://www.ai-stat.ru/ </br>
Сравнение моделей https://llmarena.ru/ </br>

---

## 2. Сравнение моделей эмбеддингов

Эмбеддинги превращают текст в векторы (наборы чисел). Без них поиск по документам невозможен.

| Критерий | Sentence-Transformers (Local) | OpenAI Embeddings (text-embedding-3-small) |
| :--- | :--- | :--- |
| **Скорость индексации** | Зависит от GPU. На RTX 3060 — очень быстро. | Ограничена лимитами API (Rate Limits) и скоростью сети. |
| **Качество поиска** | Модели типа `multilingual-e5-large` показывают отличный результат для RU/EN. | Считается эталоном для мультиязычности, но разница с локальными SOTA-моделями сокращается. |
| **Безопасность** | **Полная.** Данные не покидают сервер. | **Низкая.** Весь текст документов улетает на сервера OpenAI. |
| **Стоимость** | 0 руб. (только электричество). | Оплата за каждый миллион токенов (при объемах 15 млн/день — ощутимо). |

**Вердикт:** Для self-host решения можно выбрать модель **`intfloat/multilingual-e5-large`**. Она бесплатная и работает внутри контура.

---

## 3. Сравнение векторных баз данных

Здесь мы храним наши векторы и ищем по ним похожие куски текста.

| Критерий | FAISS | ChromaDB |
| :--- | :--- | :--- |
| **Что это?** | Библиотека для быстрого поиска в памяти. | Полноценная база данных (обертка над SQLite/DuckDB). |
| **Скорость поиска** | **Экстремальная.** Самый быстрый движок в мире для векторов. | Высокая, но ниже, чем у чистого FAISS на огромных массивах. |
| **Сложность внедрения** | **Средняя.** Нужно самому писать код для сохранения/загрузки индексов на диск. | **Низкая.** Устанавливается одной командой, имеет простой API. |
| **Удобство** | Минимум функций «из коробки». Только математика поиска. | Есть фильтрация по метаданным (например, искать только в «Приказах за 2023 год»). |
| **Стоимость (TCO)** | 0 руб. Минимальные требования к RAM. | 0 руб. Требует чуть больше ресурсов для хранения метаданных. |

**Вердикт:** 
*   Если делать **быстрый пилот** — бери **ChromaDB**. В ней проще хранить документы вместе с векторами. 
*   Если данных станет **десятки миллионов фрагментов** и нужна микросекундная скорость — переходи на **FAISS**.

---

## 4. Выбор рекомендуемой конфигурации.

Резюмируем вышесказанное по оборудованию.

||МVP|TOBE|
|-|-|-|
|**CPU**|Intel Core i5 >= 14*|Xeon x2|
|**RAM**| 16GB|512GB|
|**GPU**|NVIDIA RTX3060 8GB x1|NVIDIA H100 80GB x4|

## Итоговая рекомендация:

Для "пилотного" локального запуска можно использовать имеющееся оборудование.

1.  Эмбеддинги: `multilingual-e5-large` (локально через библиотеку `Sentence-Transformers`).
1.  Векторная база: `ChromaDB` 
1.  Интерфейс: `OLLama` 
1.  Чанкер: `Chonkie`
1.  Небольшую модель с Hugging Facе, которая поместится на локальном железе.

После отработки базовых сценариев на текущем пайплайне, нужно будет переехать в единую бд (Weaviate, Qdrant, Postgres + pgvector), и настроить в неё CI/CD для пополнения новыми данными из хранилища документации.
После переезда в бд, можно купить первый сервер c одной GPU и постепенно докупать остальные карты, по мере подключения сотрудников к поиску.